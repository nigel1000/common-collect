
# 性能优化
## Filesystem Cache
ES 的数据实际上都在磁盘文件里，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 Filesystem Cache 里面去。  
ES 的搜索引擎严重依赖于底层的 Filesystem Cache，如果给 Filesystem Cache 更多的内存，尽量让内存可以容纳所有的 IDX Segment File 索引数据文件，那么搜索的时候就基本都是走内存的，性能会非常高。     
注意点：  
- ES 的内存分配，ES JVM Heap (分配给ES项目)，Filesystem Cache 分配(堆外内存使用)。
- 只存储需要索引的字段(单条数据不要过大)，以节省内存使用空间，通过索引获取的id再去hbase或者mysql里获取全量数据。
- 写入 ES 的数据最好小于等于或者是略微大于 ES 的 Filesystem Cache 的内存容量。

## 数据预热
假设 ES 集群中每个机器写入的数据量超过了 Filesystem Cache 一倍。
可以做数据预热。在后台(缓存预热子系统)启动一个定时任务发起**热数据**的搜索请求，将数据刷到 Filesystem Cache 里。  
当用户实际上来搜索热数据的时候，就是直接从内存里搜索了。  

## Document 模型设计
ES 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。  
最好是先在系统里就完成关联，数据进行平铺，将关联好的数据直接写入 ES 中。  
这样搜索的时候，就不需要利用 ES 的搜索语法来完成 Join 之类的关联搜索了。  
ES 能支持的操作就那么多，不要考虑用 ES 做一些它不擅长的事情。如果真的有那种操作，尽量在 Document 模型设计的时候，写入的时候就完成。  
另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。  

## 分页性能优化
分布式的分页执行基本策略，假如需要每页获取10条数据。现在要查询第 100 页。  
最基本的想法是会把每个 Shard 上存储的前 10*1000 条数据都查到一个协调节点上。  
如果有 5 个 Shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。  
所以翻页的时候，翻的越深，每个 Shard 返回的数据就越多，而且协调节点处理的时间越长。所以用 ES 做分页(用mysql也一样)的时候，越翻到后面，就越是慢。  
基本解决方案：  
- 不允许深度分页
- 使用 Scroll API，会一次性生成所有数据的一个快照，每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页，基本上都是毫秒级的，但是不能随意跳到任何一页的场景。
初始化时必须指定 Scroll 参数，告诉 ES 要保存此次搜索的上下文多长时间。需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。  
- 可以用 search_after, 思路是使用前一页的结果来帮助检索下一页的数据。不允许随意翻页，只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 Sort 字段。  


